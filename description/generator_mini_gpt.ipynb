{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P-FRGyzMHRwp"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "GRYUz3QuHV6F",
        "outputId": "1e1df5fb-4f96-46b7-d85c-893ea62a8359"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b7e665f2-c741-4193-ad38-8bae2f3aaf06\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>사람이 다가오고 있습니다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>사람과 자전거가 다가오고 있습니다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>자전거와 사람이 다가오고 있습니다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>사람과 자전거가 있습니다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>자전거와 사람이 있습니다</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7e665f2-c741-4193-ad38-8bae2f3aaf06')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b7e665f2-c741-4193-ad38-8bae2f3aaf06 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b7e665f2-c741-4193-ad38-8bae2f3aaf06');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "             sentence\n",
              "0       사람이 다가오고 있습니다\n",
              "1  사람과 자전거가 다가오고 있습니다\n",
              "2  자전거와 사람이 다가오고 있습니다\n",
              "3       사람과 자전거가 있습니다\n",
              "4       자전거와 사람이 있습니다"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.read_csv('/content/captionTrain.csv')\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uScHuIyDH3KL",
        "outputId": "0150c8b7-3843-4153-ee14-a489aef61b06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문장 당 평균 길이: 4.47\n"
          ]
        }
      ],
      "source": [
        "# 문장 길이 확인\n",
        "sentence_num = 0\n",
        "\n",
        "for i, row in train.iterrows():\n",
        "  sentence = row['sentence']\n",
        "  sentence_num += len(sentence.split())\n",
        "\n",
        "print('문장 당 평균 길이: {}'.format(sentence_num / len(train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "O1EDQ3l9ItiG",
        "outputId": "04d94725-b48a-44ea-ddf2-0525fb9cf689"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.hist([len(row['sentence'].split()) for i, row in train.iterrows()], bins=50)\n",
        "plt.xlabel('length of samples')\n",
        "plt.ylabel('number of samples')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iqK1SQd-J49g"
      },
      "outputs": [],
      "source": [
        "# sentence2vec\n",
        "\n",
        "sentence_list = []\n",
        "for i, row in train.iterrows():\n",
        "  sentence = row['sentence']\n",
        "  sentence_list.append(sentence)\n",
        "\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(sentence_list)\n",
        "\n",
        "batch_size = 30\n",
        "text_ds = text_ds.shuffle(buffer_size=256)\n",
        "text_ds = text_ds.batch(batch_size)\n",
        "\n",
        "vocab_size = 100\n",
        "maxlen = 8\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=vocab_size - 1,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = maxlen + 1,\n",
        ")\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tsx08jLPLF_-",
        "outputId": "3f449ed0-eea0-4981-d606-221f9ab9801c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " '있습니다',\n",
              " '사람이',\n",
              " '탄',\n",
              " '여러',\n",
              " '사람과',\n",
              " '놓여',\n",
              " '전동킥보드가',\n",
              " '자전거가',\n",
              " '전동킥보드를',\n",
              " '자전거를',\n",
              " '볼라드가',\n",
              " '자전거',\n",
              " '오토바이가',\n",
              " '오토바이를',\n",
              " '오토바이',\n",
              " '여럿',\n",
              " '대가',\n",
              " '있고',\n",
              " '다가오고',\n",
              " '사람',\n",
              " '자동차가',\n",
              " '대와',\n",
              " '대',\n",
              " '전동킥보드와',\n",
              " '자전거와',\n",
              " '오토바이와',\n",
              " '전동킥보드',\n",
              " '다가옵니다',\n",
              " '차가',\n",
              " '차',\n",
              " '자동차와',\n",
              " '세워져',\n",
              " '명과',\n",
              " '두',\n",
              " '다수',\n",
              " '걸어갑니다',\n",
              " '차와',\n",
              " '여러개가',\n",
              " '볼라드와',\n",
              " '볼라드',\n",
              " '명이',\n",
              " '둘',\n",
              " '걸어가고',\n",
              " '폴라드',\n",
              " '타고',\n",
              " '킥보드가',\n",
              " '전동',\n",
              " '여럿과',\n",
              " '앞으로',\n",
              " '설치되어',\n",
              " '설치',\n",
              " '서',\n",
              " '사람들이',\n",
              " '명의',\n",
              " '되어',\n",
              " '늘어서']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gqrnAGt5LHLd"
      },
      "outputs": [],
      "source": [
        "def prepare_lm_inputs_labels(text):\n",
        "    \"\"\"\n",
        "    Shift word sequences by 1 position so that the target for position (i) is\n",
        "    word at position (i+1). The model will use all words up till position (i)\n",
        "    to predict the next word.\n",
        "    \"\"\"\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "    \n",
        "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
        "text_ds = text_ds.prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RgIFUWchL-On"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        # token 위치에 따른 embedding을 하기위함\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YUa31fcjMApI"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    Mask the upper half of the dot product matrix in self attention.\n",
        "    This prevents flow of information from future tokens to current token.\n",
        "    1's in the lower triangle, counting from the lower right corner.\n",
        "    \"\"\"\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "    )\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yPQwMXp1MFmk"
      },
      "outputs": [],
      "source": [
        "# 변수 정의 및 모델 함수 정의\n",
        "\n",
        "embed_dim = 256  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "def create_model():\n",
        "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32) \n",
        "    # input 정의\n",
        "    \n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim) \n",
        "    # token Embedding + positional Embedding layer class 정의\n",
        "    \n",
        "    x = embedding_layer(inputs) \n",
        "    # 선언한 Embedding layer class 이용해 Embedding\n",
        "    \n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim) \n",
        "    # transformer block layer class 정의\n",
        "    \n",
        "    x = transformer_block(x) \n",
        "    # 선언한 transformer layer class 이용해 학습\n",
        "    \n",
        "    outputs = layers.Dense(vocab_size)(x) \n",
        "    # 압축된 결과를 vocab에 맞춰 팽창 (후보단어 선별을 위한 각 단어에 대한 결과치 도출)\n",
        "    \n",
        "    model = keras.Model(inputs=inputs, outputs=[outputs, x])  \n",
        "    # model 정의\n",
        "    \n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
        "    model.compile(\n",
        "        \"adam\", loss=[loss_fn, None],\n",
        "    )  # No loss and optimization based on word embeddings from transformer block\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9wZigTmtMexD"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model.\n",
        "    1. Feed some starting prompt to the model\n",
        "    2. Predict probabilities for the next token\n",
        "    3. Sample the next token and add it to the next input\n",
        "\n",
        "    Arguments:\n",
        "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
        "        start_tokens: List of integers, the token indices for the starting prompt.\n",
        "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
        "        top_k: Integer, sample from the `top_k` token predictions.\n",
        "        print_every: Integer, print after this many epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
        "    ):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.start_tokens = start_tokens\n",
        "        self.index_to_word = index_to_word\n",
        "        self.print_every = print_every\n",
        "        self.k = top_k\n",
        "\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
        "        indices = np.asarray(indices).astype(\"int32\")\n",
        "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "        return np.random.choice(indices, p=preds)\n",
        "\n",
        "    def detokenize(self, number):\n",
        "        return self.index_to_word[number]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        start_tokens = [_ for _ in self.start_tokens]\n",
        "        if (epoch + 1) % self.print_every != 0:\n",
        "            return\n",
        "        num_tokens_generated = 0\n",
        "        tokens_generated = []\n",
        "        while num_tokens_generated <= self.max_tokens:\n",
        "            pad_len = maxlen - len(start_tokens)\n",
        "            sample_index = len(start_tokens) - 1\n",
        "            if pad_len < 0:\n",
        "                x = start_tokens[:maxlen]\n",
        "                sample_index = maxlen - 1\n",
        "            elif pad_len > 0:\n",
        "                x = start_tokens + [0] * pad_len\n",
        "            else:\n",
        "                x = start_tokens\n",
        "            x = np.array([x])\n",
        "            y, _ = self.model.predict(x)\n",
        "            sample_token = self.sample_from(y[0][sample_index])\n",
        "            tokens_generated.append(sample_token)\n",
        "            start_tokens.append(sample_token)\n",
        "            num_tokens_generated = len(tokens_generated)\n",
        "        txt = \" \".join(\n",
        "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
        "        )\n",
        "        generated_sentence.append(txt)\n",
        "        print()\n",
        "        print(f\"generated text: {txt}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPbM71uFMraZ",
        "outputId": "691e668e-42d3-4d5a-d764-16e679dcaf21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 8)]               0         \n",
            "                                                                 \n",
            " token_and_position_embeddin  (None, 8, 256)           27648     \n",
            " g_3 (TokenAndPositionEmbedd                                     \n",
            " ing)                                                            \n",
            "                                                                 \n",
            " transformer_block_3 (Transf  (None, 8, 256)           658688    \n",
            " ormerBlock)                                                     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 8, 100)            25700     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 712,036\n",
            "Trainable params: 712,036\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 3.2286 - dense_11_loss: 3.2286\n",
            "generated text: 자전거 탄 있습니다                                       \n",
            "\n",
            "4/4 [==============================] - 6s 916ms/step - loss: 3.0692 - dense_11_loss: 3.0692\n",
            "Epoch 2/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 1.2105 - dense_11_loss: 1.2105\n",
            "generated text: 자전거 여러 있고 사람이 있습니다                                     \n",
            "\n",
            "4/4 [==============================] - 2s 750ms/step - loss: 1.1636 - dense_11_loss: 1.1636\n",
            "Epoch 3/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.8653 - dense_11_loss: 0.8653\n",
            "generated text: 자전거 여러 대와 사람이 걸어갑니다                                     \n",
            "\n",
            "4/4 [==============================] - 2s 744ms/step - loss: 0.8570 - dense_11_loss: 0.8570\n",
            "Epoch 4/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.6455 - dense_11_loss: 0.6455\n",
            "generated text: 자전거 여러 대와 다가오고 있습니다                                     \n",
            "\n",
            "4/4 [==============================] - 2s 765ms/step - loss: 0.6618 - dense_11_loss: 0.6618\n",
            "Epoch 5/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.5737 - dense_11_loss: 0.5737\n",
            "generated text: 자전거 차 오토바이 여러 대가 있습니다                                    \n",
            "\n",
            "4/4 [==============================] - 2s 745ms/step - loss: 0.5629 - dense_11_loss: 0.5629\n",
            "Epoch 6/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.4767 - dense_11_loss: 0.4767\n",
            "generated text: 자전거 여러 대와 사람이 있습니다                                     \n",
            "\n",
            "4/4 [==============================] - 2s 732ms/step - loss: 0.4842 - dense_11_loss: 0.4842\n",
            "Epoch 7/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.4379 - dense_11_loss: 0.4379\n",
            "generated text: 자전거 사람 여러 대가 있습니다                                     \n",
            "\n",
            "4/4 [==============================] - 2s 742ms/step - loss: 0.4398 - dense_11_loss: 0.4398\n",
            "Epoch 8/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.3992 - dense_11_loss: 0.3992\n",
            "generated text: 자전거 여러 대와 사람이 있습니다                                     \n",
            "\n",
            "4/4 [==============================] - 2s 754ms/step - loss: 0.3932 - dense_11_loss: 0.3932\n",
            "Epoch 9/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.3681 - dense_11_loss: 0.3681\n",
            "generated text: 자전거 여러 대가 세워져 있습니다                                     \n",
            "\n",
            "4/4 [==============================] - 2s 742ms/step - loss: 0.3677 - dense_11_loss: 0.3677\n",
            "Epoch 10/10\n",
            "3/4 [=====================>........] - ETA: 0s - loss: 0.3343 - dense_11_loss: 0.3343\n",
            "generated text: 자전거 여러 대와 전동 킥보드가 있습니다                                    \n",
            "\n",
            "4/4 [==============================] - 3s 853ms/step - loss: 0.3455 - dense_11_loss: 0.3455\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f06a1681ed0>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tokenize starting prompt\n",
        "word_to_index = {}\n",
        "for index, word in enumerate(vocab):\n",
        "    word_to_index[word] = index\n",
        "\n",
        "start_prompt = \"자전거\"\n",
        "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "num_tokens_generated = 40\n",
        "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)\n",
        "\n",
        "generated_sentence = []\n",
        "model = create_model()\n",
        "model.fit(text_ds, verbose=1, epochs=10, callbacks=[text_gen_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "S3pDssUbl_zq",
        "outputId": "9696da20-430a-4357-fd5a-68074ee47047"
      },
      "outputs": [
        {
          "data": {
            "image/svg+xml": "<svg height=\"274pt\" viewBox=\"0.00 0.00 580.00 304.00\" width=\"524pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.9028 .9028) rotate(0) translate(4 300)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-300 576,-300 576,4 -4,4\" stroke=\"transparent\"/>\n<!-- 139666419710416 -->\n<g class=\"node\" id=\"node1\">\n<title>139666419710416</title>\n<polygon fill=\"none\" points=\"145.5,-249.5 145.5,-295.5 426.5,-295.5 426.5,-249.5 145.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"176.5\" y=\"-268.8\">input_4</text>\n<polyline fill=\"none\" points=\"207.5,-249.5 207.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"247.5\" y=\"-268.8\">InputLayer</text>\n<polyline fill=\"none\" points=\"287.5,-249.5 287.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"316.5\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"287.5,-272.5 345.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"316.5\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"345.5,-249.5 345.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"386\" y=\"-280.3\">[(None, 8)]</text>\n<polyline fill=\"none\" points=\"345.5,-272.5 426.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"386\" y=\"-257.3\">[(None, 8)]</text>\n</g>\n<!-- 139666419711632 -->\n<g class=\"node\" id=\"node2\">\n<title>139666419711632</title>\n<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 572,-212.5 572,-166.5 0,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"109\" y=\"-185.8\">token_and_position_embedding_3</text>\n<polyline fill=\"none\" points=\"218,-166.5 218,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"315\" y=\"-185.8\">TokenAndPositionEmbedding</text>\n<polyline fill=\"none\" points=\"412,-166.5 412,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"441\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"412,-189.5 470,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"441\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"470,-166.5 470,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"521\" y=\"-197.3\">(None, 8)</text>\n<polyline fill=\"none\" points=\"470,-189.5 572,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"521\" y=\"-174.3\">(None, 8, 256)</text>\n</g>\n<!-- 139666419710416&#45;&gt;139666419711632 -->\n<g class=\"edge\" id=\"edge1\">\n<title>139666419710416-&gt;139666419711632</title>\n<path d=\"M286,-249.3799C286,-241.1745 286,-231.7679 286,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"289.5001,-222.784 286,-212.784 282.5001,-222.784 289.5001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139666461754192 -->\n<g class=\"node\" id=\"node3\">\n<title>139666461754192</title>\n<polygon fill=\"none\" points=\"77,-83.5 77,-129.5 495,-129.5 495,-83.5 77,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"145.5\" y=\"-102.8\">transformer_block_3</text>\n<polyline fill=\"none\" points=\"214,-83.5 214,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"274.5\" y=\"-102.8\">TransformerBlock</text>\n<polyline fill=\"none\" points=\"335,-83.5 335,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"364\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"335,-106.5 393,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"364\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"393,-83.5 393,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"444\" y=\"-114.3\">(None, 8, 256)</text>\n<polyline fill=\"none\" points=\"393,-106.5 495,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"444\" y=\"-91.3\">(None, 8, 256)</text>\n</g>\n<!-- 139666419711632&#45;&gt;139666461754192 -->\n<g class=\"edge\" id=\"edge2\">\n<title>139666419711632-&gt;139666461754192</title>\n<path d=\"M286,-166.3799C286,-158.1745 286,-148.7679 286,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"289.5001,-139.784 286,-129.784 282.5001,-139.784 289.5001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 139666386064720 -->\n<g class=\"node\" id=\"node4\">\n<title>139666386064720</title>\n<polygon fill=\"none\" points=\"144.5,-.5 144.5,-46.5 427.5,-46.5 427.5,-.5 144.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"180\" y=\"-19.8\">dense_11</text>\n<polyline fill=\"none\" points=\"215.5,-.5 215.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"241.5\" y=\"-19.8\">Dense</text>\n<polyline fill=\"none\" points=\"267.5,-.5 267.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"296.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"267.5,-23.5 325.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"296.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"325.5,-.5 325.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"376.5\" y=\"-31.3\">(None, 8, 256)</text>\n<polyline fill=\"none\" points=\"325.5,-23.5 427.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"376.5\" y=\"-8.3\">(None, 8, 100)</text>\n</g>\n<!-- 139666461754192&#45;&gt;139666386064720 -->\n<g class=\"edge\" id=\"edge3\">\n<title>139666461754192-&gt;139666386064720</title>\n<path d=\"M286,-83.3799C286,-75.1745 286,-65.7679 286,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"289.5001,-56.784 286,-46.784 282.5001,-56.784 289.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>",
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "SVG(model_to_dot(model, show_shapes=True, dpi=65).create(prog='dot', format='svg'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNbIWP_jfg58",
        "outputId": "7960a008-45aa-4b8b-fe20-0982ac35b13c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['자전거 탄 있습니다',\n",
              " '자전거 여러 있고 사람이 있습니다',\n",
              " '자전거 여러 대와 사람이 걸어갑니다',\n",
              " '자전거 여러 대와 다가오고 있습니다',\n",
              " '자전거 차 오토바이 여러 대가 있습니다',\n",
              " '자전거 여러 대와 사람이 있습니다',\n",
              " '자전거 사람 여러 대가 있습니다',\n",
              " '자전거 여러 대와 사람이 있습니다',\n",
              " '자전거 여러 대가 세워져 있습니다',\n",
              " '자전거 여러 대와 전동 킥보드가 있습니다']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generated_sentence_list = [senten.rstrip() for senten in generated_sentence]\n",
        "generated_sentence_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-NuRL-INb-z"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "sentenceGenerator_miniGPT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
